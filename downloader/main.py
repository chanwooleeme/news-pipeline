import os
import json
import hashlib
import feedparser
import requests
import random
import time
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set
from redis import Redis
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
from prometheus_client import CollectorRegistry, Counter, Gauge, push_to_gateway

# ============================================================
# ğŸ§© ë¡œê¹… ì„¤ì •
# ============================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ============================================================
# ğŸ§© Prometheus ë©”íŠ¸ë¦­ (ë…ë¦½ ë ˆì§€ìŠ¤íŠ¸ë¦¬)
# ============================================================
registry = CollectorRegistry()

rss_fetch_failures_total = Counter(
    "rss_fetch_failures_total",
    "RSS fetch failures",
    registry=registry
)
download_success_total = Counter(
    "download_success_total",
    "Download success count",
    registry=registry
)
download_failure_total = Counter(
    "download_failure_total",
    "Download failure count",
    registry=registry
)
redis_items_pushed_total = Counter(
    "redis_items_pushed_total",
    "Items pushed to Redis",
    registry=registry
)
pipeline_batch_duration_seconds = Gauge(
    "pipeline_batch_duration_seconds",
    "Total pipeline duration",
    registry=registry
)

# ============================================================
# ğŸ§© STEP 0. ê³µí†µ ì„¤ì •
# ============================================================

class DownloaderConfig:
    """ë‹¤ìš´ë¡œë” ì „ì—­ ì„¤ì •"""
    def __init__(self):
        # Redis ì—°ê²° ë° ê¸°ë³¸ TTL
        self.redis_url = os.getenv('REDIS_URL', 'redis://localhost:6379')
        self.redis_ttl = 72 * 3600  # 72ì‹œê°„ ë™ì•ˆ ì¤‘ë³µ ì²´í¬ ìœ ì§€
        
        # HTTP ìš”ì²­ ì„¤ì •
        self.user_agent = (
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )
        self.timeout = 10

        # ThreadPool ë³‘ë ¬ ê°œìˆ˜
        self.max_workers = int(os.getenv('MAX_WORKERS', '12'))

        # Docker / Local í™˜ê²½ì— ë”°ë¼ ê²½ë¡œ ë¶„ê¸°
        self.is_docker = os.getenv('IS_DOCKER', '0') == '1'
        self.base_path = Path('/app') if self.is_docker else Path(__file__).parent.parent
        self.rss_config_path = self.base_path / 'shared' / 'rss_feeds.json'
        self.temp_base_dir = self.base_path / 'temp'
        
        # Pushgateway ì„¤ì •
        self.pushgateway_url = os.getenv('PUSHGATEWAY_URL', 'localhost:9091')


# ============================================================
# ğŸ§© STEP 1. RSS ìˆ˜ì§‘ê¸° (feedparser)
# ============================================================

class RssFetcher:
    """RSS í”¼ë“œ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, config: DownloaderConfig):
        self.config = config

    def load_feeds(self) -> Dict[str, List[str]]:
        """rss_feeds.json ë¡œë“œ (ì£¼ì„ ì œê±° í›„ íŒŒì‹±)"""
        with open(self.config.rss_config_path, 'r', encoding='utf-8') as f:
            raw_text = f.read().lstrip('\ufeff')
            lines = [
                line for line in raw_text.splitlines()
                if line.strip() and not line.strip().startswith('//')
            ]
            return json.loads("\n".join(lines))
    
    def fetch_urls(self, feed_url: str) -> List[str]:
        """ë‹¨ì¼ RSS í”¼ë“œì—ì„œ URL ëª©ë¡ ìˆ˜ì§‘"""
        try:
            # (íŠ¹ì • ë„ë©”ì¸ ì˜ˆì™¸ ì²˜ë¦¬)
            if "pressian.com" in feed_url:
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0)",
                    "Accept": "application/xml,text/html,application/xhtml+xml"
                }
                resp = requests.get(feed_url, headers=headers, timeout=(5, 60))
                resp.raise_for_status()
                feed = feedparser.parse(resp.content)
            else:
                feed = feedparser.parse(feed_url)
            return [entry.link for entry in feed.entries if hasattr(entry, "link")]
        except Exception as e:
            logger.warning(f"RSS íŒŒì‹± ì‹¤íŒ¨: {feed_url[:60]} - {e}")
            rss_fetch_failures_total.inc()
            return []


# ============================================================
# ğŸ§© STEP 2. Redis ê´€ë¦¬ (ì¤‘ë³µ ì²´í¬ + Burst ì—…ë°ì´íŠ¸)
# ============================================================

class RedisManager:
    """Redis ê´€ë¦¬ (burst insert ìµœì í™”)"""
    
    def __init__(self, config: DownloaderConfig):
        self.redis = Redis.from_url(
            config.redis_url,
            decode_responses=False,
            socket_timeout=10,           # 10ì´ˆ ì½ê¸° íƒ€ì„ì•„ì›ƒ
            socket_connect_timeout=5,    # 5ì´ˆ ì—°ê²° íƒ€ì„ì•„ì›ƒ
            socket_keepalive=True,       # ì—°ê²° ìœ ì§€
            health_check_interval=30     # 30ì´ˆë§ˆë‹¤ ì—°ê²° í™•ì¸
        )
        self.ttl = config.redis_ttl

    def get_existing_hashes(self, url_hashes: Set[str]) -> Set[str]:
        """ì´ë¯¸ Redisì— ì¡´ì¬í•˜ëŠ” URL í•´ì‹œ ì¡°íšŒ"""
        try:
            pipe = self.redis.pipeline()
            for h in url_hashes:
                pipe.exists(f"url:{h}")
            results = pipe.execute()
            return {h for h, exists in zip(url_hashes, results) if exists}
        except Exception as e:
            logger.error(f"Redis í•´ì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return set()

    def flush_burst(self, tasks: List[dict]) -> None:
        """ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ Redisì— ë°°ì¹˜ ì‚½ì…"""
        if not tasks:
            return
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                pipe = self.redis.pipeline()
                for t in tasks:
                    pipe.lpush("parse_queue", json.dumps(t))
                    pipe.setex(f"url:{t['url_hash']}", self.ttl, "1")
                pipe.execute()
                logger.info(f"Redis batch ì €ì¥ ì™„ë£Œ ({len(tasks)}ê±´)")
                redis_items_pushed_total.inc(len(tasks))
                return
            except Exception as e:
                logger.warning(f"Redis flush ì‹¤íŒ¨ (attempt {attempt+1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(1)
                else:
                    logger.error(f"Redis flush ìµœì¢… ì‹¤íŒ¨ after {max_retries} attempts")
                    raise

    def send_batch_complete_signal(self, timestamp: str, total_tasks: int) -> None:
        """ì „ì²´ ë°°ì¹˜ ì™„ë£Œ ì‹ í˜¸"""
        try:
            signal_data = {
                'timestamp': timestamp,
                'total_tasks': total_tasks,
                'completed_at': datetime.now().isoformat()
            }
            self.redis.setex(f'batch_complete:{timestamp}', 3600, json.dumps(signal_data))
            logger.info(f"ë°°ì¹˜ ì™„ë£Œ ì‹ í˜¸ ì „ì†¡: {timestamp} ({total_tasks}ê±´)")
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì™„ë£Œ ì‹ í˜¸ ì „ì†¡ ì‹¤íŒ¨: {e}")


# ============================================================
# STEP 3. HTML ë‹¤ìš´ë¡œë” (Thread ê¸°ë°˜ polite crawler)
# ============================================================

class HtmlDownloader:
    """HTML ë‹¤ìš´ë¡œë“œ + íŒŒì¼ ì €ì¥"""
    
    def __init__(self, config: DownloaderConfig, temp_dir: Path):
        self.config = config
        self.temp_dir = temp_dir

    @staticmethod
    def generate_hash(url: str) -> str:
        return hashlib.md5(url.encode()).hexdigest()

    def download(self, url: str) -> bytes:
        response = requests.get(
            url,
            headers={'User-Agent': self.config.user_agent},
            timeout=self.config.timeout
        )
        response.raise_for_status()
        return response.content

    def save(self, publisher: str, url_hash: str, html: bytes) -> Path:
        """HTML íŒŒì¼ ë¡œì»¬ ì €ì¥"""
        filepath = self.temp_dir / publisher / f"{url_hash}.html"
        filepath.parent.mkdir(parents=True, exist_ok=True)
        filepath.write_bytes(html)
        return filepath


# ============================================================
# STEP 4. ë‰´ìŠ¤ ë‹¤ìš´ë¡œë“œ íŒŒì´í”„ë¼ì¸
# ============================================================

class NewsDownloadPipeline:
    """ë‰´ìŠ¤ ë‹¤ìš´ë¡œë“œ íŒŒì´í”„ë¼ì¸ (ThreadPool + Redis Burst)"""
    
    def __init__(self, timestamp: str):
        self.timestamp = timestamp
        self.config = DownloaderConfig()
        self.temp_dir = self.config.temp_base_dir / 'html' / timestamp
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        self.test_mode = os.getenv("TEST_MODE", "false").lower() == "true"
        self.rss_fetcher = RssFetcher(self.config)
        self.html_downloader = HtmlDownloader(self.config, self.temp_dir)
        self.redis_manager = RedisManager(self.config)

        logger.info(f"ì €ì¥ ê²½ë¡œ: {self.temp_dir}")
        logger.info(f"RSS ì„¤ì •: {self.config.rss_config_path}")
        logger.info(f"Worker ìˆ˜: {self.config.max_workers}")
        logger.info(f"í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {self.test_mode}")

    def collect_urls(self) -> List[dict]:
        """RSSì—ì„œ ì‹ ê·œ URL ìˆ˜ì§‘"""
        rss_feeds = self.rss_fetcher.load_feeds()
        logger.info(f"{len(rss_feeds)}ê°œ ì–¸ë¡ ì‚¬ RSS ë¡œë“œ ì™„ë£Œ")

        # 1ï¸âƒ£ ëª¨ë“  RSSë¥¼ í•œ ë²ˆë§Œ í˜¸ì¶œ
        all_urls_by_publisher = {}
        for publisher, feed_urls in rss_feeds.items():
            urls = []
            for feed_url in feed_urls:
                fetched = self.rss_fetcher.fetch_urls(feed_url)
                urls.extend(fetched)
            all_urls_by_publisher[publisher] = urls

        # 2ï¸âƒ£ ì „ì²´ URL set ìƒì„± (ì¤‘ë³µ ì œê±°)
        all_urls = {u for urls in all_urls_by_publisher.values() for u in urls}

        # 3ï¸âƒ£ Redisì—ì„œ ê¸°ì¡´ í•´ì‹œ í™•ì¸
        url_hashes = {HtmlDownloader.generate_hash(u) for u in all_urls}
        existing = self.redis_manager.get_existing_hashes(url_hashes)

        # 4ï¸âƒ£ ì‹ ê·œ URLë§Œ í•„í„°ë§
        new_count = len(all_urls) - len(existing)
        logger.info(f"ì‹ ê·œ URL {new_count}ê°œ (ì¤‘ë³µ {len(existing)}ê°œ ì œì™¸)")

        # 5ï¸âƒ£ publisher ì •ë³´ ë§¤í•‘
        url_pool = []
        for publisher, urls in all_urls_by_publisher.items():
            for url in urls:
                url_hash = HtmlDownloader.generate_hash(url)
                if url_hash not in existing:
                    url_pool.append({
                        "publisher": publisher,
                        "url": url,
                        "url_hash": url_hash
                    })

        # 6ï¸âƒ£ íŠ¸ë˜í”½ ë¶„ì‚° + í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì²˜ë¦¬
        random.shuffle(url_pool)
        if self.test_mode:
            url_pool = url_pool[:10]
            logger.warning(f"[TEST MODE] URL 10ê°œë§Œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤")

        return url_pool

    def download_task(self, item: dict) -> dict | None:
        """ë‹¨ì¼ URL ë‹¤ìš´ë¡œë“œ (polite crawling)"""
        publisher, url, url_hash = item["publisher"], item["url"], item["url_hash"]
        try:
            time.sleep(random.uniform(0.1, 0.5))  # í´ë¼ì´íŠ¸ í¬ë¡¤ë§
            html = self.html_downloader.download(url)
            filepath = self.html_downloader.save(publisher, url_hash, html)
            download_success_total.inc()
            return {
                "timestamp": self.timestamp,
                "publisher": publisher,
                "url": url,
                "filepath": str(filepath),
                "url_hash": url_hash
            }
        except Exception as e:
            download_failure_total.inc()
            logger.debug(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url[:80]} - {e}")
            return None

    def run(self) -> None:
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info(f"ë‹¤ìš´ë¡œë” ì‹œì‘: {self.timestamp}")
        logger.info(f"í™˜ê²½: {'Docker' if self.config.is_docker else 'ë¡œì»¬'}")

        start_time = time.perf_counter()
        url_pool = self.collect_urls()
        
        if not url_pool:
            logger.info("ë‹¤ìš´ë¡œë“œí•  ì‹ ê·œ URLì´ ì—†ìŠµë‹ˆë‹¤")
            return

        pending_tasks = []
        downloaded = 0
        failed = 0

        # ThreadPool ì‹¤í–‰ (ì „ì²´ íƒ€ì„ì•„ì›ƒ 1ì‹œê°„)
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            futures = {executor.submit(self.download_task, item): item for item in url_pool}
            
            try:
                for future in as_completed(futures, timeout=3600):
                    try:
                        result = future.result(timeout=30)  # ê°œë³„ ì‘ì—… 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                        if result:
                            pending_tasks.append(result)
                            downloaded += 1
                            
                            # 500ê°œ ë‹¨ìœ„ë¡œ Redis flush
                            if len(pending_tasks) >= 500:
                                self.redis_manager.flush_burst(pending_tasks)
                                pending_tasks.clear()
                        else:
                            failed += 1
                            
                    except TimeoutError:
                        failed += 1
                        item = futures[future]
                        logger.warning(f"ì‘ì—… íƒ€ì„ì•„ì›ƒ: {item['url'][:60]}")
                    except Exception as e:
                        failed += 1
                        logger.error(f"ì‘ì—… ì‹¤íŒ¨: {e}")
                    
                    # ì§„í–‰ ìƒí™© ì¶œë ¥ (20ê°œë§ˆë‹¤)
                    total_done = downloaded + failed
                    if total_done % 20 == 0:
                        logger.info(f"ì§„í–‰ë¥ : {total_done}/{len(url_pool)} (ì„±ê³µ:{downloaded}, ì‹¤íŒ¨:{failed})")
                        
            except TimeoutError:
                logger.error("ì „ì²´ ì‘ì—… íƒ€ì„ì•„ì›ƒ (1ì‹œê°„ ì´ˆê³¼)")

        # ë‚¨ì€ ë°ì´í„° ìµœì¢… flush
        if pending_tasks:
            self.redis_manager.flush_burst(pending_tasks)

        elapsed = time.perf_counter() - start_time
        pipeline_batch_duration_seconds.set(elapsed)

        # ë°°ì¹˜ ì™„ë£Œ ì‹ í˜¸
        if downloaded > 0:
            self.redis_manager.send_batch_complete_signal(self.timestamp, downloaded)

        # ê²°ê³¼ ìš”ì•½
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š ê²°ê³¼ ìš”ì•½")
        logger.info(f"  ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {downloaded}ê°œ")
        logger.info(f"  ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {failed}ê°œ")
        logger.info(f"  ì´ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        if downloaded > 0:
            logger.info(f"  í‰ê·  ì²˜ë¦¬ ì†ë„: {downloaded / elapsed:.2f}ê±´/ì´ˆ")
        logger.info("=" * 60)
        
        # Pushgatewayë¡œ ë©”íŠ¸ë¦­ ì „ì†¡
        try:
            push_to_gateway(
                self.config.pushgateway_url,
                job='news_downloader',
                grouping_key={'timestamp': self.timestamp},
                registry=registry
            )
            logger.info(f"ë©”íŠ¸ë¦­ ì „ì†¡ ì™„ë£Œ: {self.config.pushgateway_url}")
        except Exception as e:
            logger.error(f"ë©”íŠ¸ë¦­ ì „ì†¡ ì‹¤íŒ¨: {e}")


# ============================================================
# ENTRY POINT
# ============================================================

def main():
    timestamp = datetime.now().strftime("%Y%m%d%H")
    pipeline = NewsDownloadPipeline(timestamp)
    pipeline.run()


if __name__ == "__main__":
    main()