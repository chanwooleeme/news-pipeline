import os
import json
import hashlib
import feedparser
import requests
import random
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set
from redis import Redis
from concurrent.futures import ThreadPoolExecutor, as_completed
from prometheus_client import Counter, Summary, Gauge, start_http_server

rss_fetch_failures_total = Counter("rss_fetch_failures_total", "RSS fetch failures")
download_success_total = Counter("download_success_total", "Download success count")
download_failure_total = Counter("download_failure_total", "Download failure count")
redis_items_pushed_total = Counter("redis_items_pushed_total", "Items pushed to Redis")
pipeline_batch_duration_seconds = Gauge("pipeline_batch_duration_seconds", "Total pipeline duration")


# ============================================================
# ğŸ§© STEP 0. ê³µí†µ ì„¤ì •
# ============================================================

class DownloaderConfig:
    """ë‹¤ìš´ë¡œë” ì „ì—­ ì„¤ì •"""
    def __init__(self):
        # Redis ì—°ê²° ë° ê¸°ë³¸ TTL
        self.redis_url = os.getenv('REDIS_URL', 'redis://localhost:6379')
        self.redis_ttl = 72 * 3600  # 72ì‹œê°„ ë™ì•ˆ ì¤‘ë³µ ì²´í¬ ìœ ì§€
        
        # HTTP ìš”ì²­ ì„¤ì •
        self.user_agent = (
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )
        self.timeout = 10

        # ThreadPool ë³‘ë ¬ ê°œìˆ˜
        self.max_workers = int(os.getenv('MAX_WORKERS', '12'))

        # Docker / Local í™˜ê²½ì— ë”°ë¼ ê²½ë¡œ ë¶„ê¸°
        self.is_docker = os.getenv('IS_DOCKER', '0') == '1'
        self.base_path = Path('/app') if self.is_docker else Path(__file__).parent.parent
        self.rss_config_path = self.base_path / 'shared' / 'rss_feeds.json'
        self.temp_base_dir = self.base_path / 'temp'


# ============================================================
# ğŸ§© STEP 1. RSS ìˆ˜ì§‘ê¸° (feedparser)
# ============================================================

class RssFetcher:
    """RSS í”¼ë“œ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, config: DownloaderConfig):
        self.config = config

    def load_feeds(self) -> Dict[str, List[str]]:
        """rss_feeds.json ë¡œë“œ (ì£¼ì„ ì œê±° í›„ íŒŒì‹±)"""
        with open(self.config.rss_config_path, 'r', encoding='utf-8') as f:
            raw_text = f.read().lstrip('\ufeff')
            lines = [
                line for line in raw_text.splitlines()
                if line.strip() and not line.strip().startswith('//')
            ]
            return json.loads("\n".join(lines))
    
    def fetch_urls(self, feed_url: str) -> List[str]:
        """ë‹¨ì¼ RSS í”¼ë“œì—ì„œ URL ëª©ë¡ ìˆ˜ì§‘"""
        try:
            # (íŠ¹ì • ë„ë©”ì¸ ì˜ˆì™¸ ì²˜ë¦¬)
            if "pressian.com" in feed_url:
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0)",
                    "Accept": "application/xml,text/html,application/xhtml+xml"
                }
                resp = requests.get(feed_url, headers=headers, timeout=(5, 60))
                resp.raise_for_status()
                feed = feedparser.parse(resp.content)
            else:
                feed = feedparser.parse(feed_url)
            return [entry.link for entry in feed.entries if hasattr(entry, "link")]
        except Exception as e:
            print(f"  âœ— RSS íŒŒì‹± ì‹¤íŒ¨: {feed_url[:60]} - {e}")
            return []


# ============================================================
# ğŸ§© STEP 2. Redis ê´€ë¦¬ (ì¤‘ë³µ ì²´í¬ + Burst ì—…ë°ì´íŠ¸)
# ============================================================

class RedisManager:
    """Redis ê´€ë¦¬ (burst insert ìµœì í™”)"""
    
    def __init__(self, config: DownloaderConfig):
        self.redis = Redis.from_url(config.redis_url, decode_responses=False)
        self.ttl = config.redis_ttl

    def get_existing_hashes(self, url_hashes: Set[str]) -> Set[str]:
        """ì´ë¯¸ Redisì— ì¡´ì¬í•˜ëŠ” URL í•´ì‹œ ì¡°íšŒ"""
        pipe = self.redis.pipeline()
        for h in url_hashes:
            pipe.exists(f"url:{h}")
        results = pipe.execute()
        return {h for h, exists in zip(url_hashes, results) if exists}

    def flush_burst(self, tasks: List[dict]) -> None:
        """ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ Redisì— ë°°ì¹˜ ì‚½ì…"""
        # TODO: redis ê²½ëŸ‰í™”. ê²½ë¡œë§Œ ë„£ê³  parserì—ì„œ íŒŒì‹±.
        if not tasks:
            return
        pipe = self.redis.pipeline()
        for t in tasks:
            pipe.lpush("parse_queue", json.dumps(t))
            pipe.setex(f"url:{t['url_hash']}", self.ttl, "1")
        pipe.execute()
        print(f"ğŸ’¾ Redis batch ì €ì¥ ì™„ë£Œ ({len(tasks)}ê±´)")

    def send_batch_complete_signal(self, timestamp: str, total_tasks: int) -> None:
        """ì „ì²´ ë°°ì¹˜ ì™„ë£Œ ì‹ í˜¸"""
        signal_data = {
            'timestamp': timestamp,
            'total_tasks': total_tasks,
            'completed_at': datetime.now().isoformat()
        }
        self.redis.setex(f'batch_complete:{timestamp}', 3600, json.dumps(signal_data))


# ============================================================
# STEP 3. HTML ë‹¤ìš´ë¡œë” (Thread ê¸°ë°˜ polite crawler)
# ============================================================

class HtmlDownloader:
    """HTML ë‹¤ìš´ë¡œë“œ + íŒŒì¼ ì €ì¥"""
    
    def __init__(self, config: DownloaderConfig, temp_dir: Path):
        self.config = config
        self.temp_dir = temp_dir

    @staticmethod
    def generate_hash(url: str) -> str:
        return hashlib.md5(url.encode()).hexdigest()

    def download(self, url: str) -> bytes:
        response = requests.get(
            url,
            headers={'User-Agent': self.config.user_agent},
            timeout=self.config.timeout
        )
        response.raise_for_status()
        return response.content

    def save(self, publisher: str, url_hash: str, html: bytes) -> Path:
        """HTML íŒŒì¼ ë¡œì»¬ ì €ì¥"""
        filepath = self.temp_dir / publisher / f"{url_hash}.html"
        filepath.parent.mkdir(parents=True, exist_ok=True)
        filepath.write_bytes(html)
        return filepath


# ============================================================
# STEP 4. ë‰´ìŠ¤ ë‹¤ìš´ë¡œë“œ íŒŒì´í”„ë¼ì¸
# ============================================================

class NewsDownloadPipeline:
    """ë‰´ìŠ¤ ë‹¤ìš´ë¡œë“œ íŒŒì´í”„ë¼ì¸ (ThreadPool + Redis Burst)"""
    
    def __init__(self, timestamp: str):
        self.timestamp = timestamp
        self.config = DownloaderConfig()
        self.temp_dir = self.config.temp_base_dir / 'html' / timestamp
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        self.test_mode = os.getenv("TEST_MODE", "false").lower() == "true"
        self.rss_fetcher = RssFetcher(self.config)
        self.html_downloader = HtmlDownloader(self.config, self.temp_dir)
        self.redis_manager = RedisManager(self.config)

        print(f"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {self.temp_dir}")
        print(f"ğŸ“‚ RSS ì„¤ì •: {self.config.rss_config_path}")


    # ----------------------------------------------------------
    # STEP 1~4 ìš”ì•½
    # 1. feedparserë¡œ RSS URL ëª¨ë‘ ìˆ˜ì§‘
    # 2. URL ì „ì²´ set ìƒì„± â†’ ì¤‘ë³µ ìë™ ì œê±°
    # 3. Redisì— TTL(72ì‹œê°„)ë¡œ ë“±ë¡ëœ ê¸°ì¡´ URL í•´ì‹œ ì¡°íšŒ
    # 4. Redisì— ì¡´ì¬í•˜ëŠ” URL ì œì™¸í•˜ì—¬ ì‹ ê·œ URLë§Œ ë‚¨ê¹€
    # ----------------------------------------------------------
    def collect_urls(self) -> List[dict]:
        rss_feeds = self.rss_fetcher.load_feeds()
        print(f"ğŸ“° {len(rss_feeds)}ê°œ ì–¸ë¡ ì‚¬ RSS ë¡œë“œ ì™„ë£Œ")

        # âœ… 1ï¸âƒ£ ëª¨ë“  RSSë¥¼ í•œ ë²ˆë§Œ í˜¸ì¶œí•˜ê³  ê²°ê³¼ ì €ì¥
        all_urls_by_publisher = {}

        for publisher, feed_urls in rss_feeds.items():
            urls = []
            for feed_url in feed_urls:
                fetched = self.rss_fetcher.fetch_urls(feed_url)
                urls.extend(fetched)
            all_urls_by_publisher[publisher] = urls

        # âœ… 2ï¸âƒ£ ì „ì²´ URL set ìƒì„± (ì¤‘ë³µ ì œê±°)
        all_urls = {u for urls in all_urls_by_publisher.values() for u in urls}

        # âœ… 3ï¸âƒ£ Redisì—ì„œ ê¸°ì¡´ í•´ì‹œ í™•ì¸
        url_hashes = {HtmlDownloader.generate_hash(u) for u in all_urls}
        existing = self.redis_manager.get_existing_hashes(url_hashes)

        # âœ… 4ï¸âƒ£ ì‹ ê·œ URLë§Œ í•„í„°ë§
        print(f"ğŸ“¥ ì‹ ê·œ URL {len(all_urls) - len(existing)}ê°œ (ì¤‘ë³µ {len(existing)}ê°œ ì œì™¸)")

        # âœ… 5ï¸âƒ£ publisher ì •ë³´ ë§¤í•‘ (ì´ë¯¸ ë¡œë“œí•œ ë°ì´í„° ì¬í™œìš©)
        url_pool = []
        for publisher, urls in all_urls_by_publisher.items():
            for url in urls:
                url_hash = HtmlDownloader.generate_hash(url)
                if url_hash not in existing:
                    url_pool.append({
                        "publisher": publisher,
                        "url": url,
                        "url_hash": url_hash
                    })

        # âœ… 6ï¸âƒ£ íŠ¸ë˜í”½ ë¶„ì‚° + í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì²˜ë¦¬
        random.shuffle(url_pool)
        if self.test_mode:
            url_pool = url_pool[:10]
            print(f"ğŸ§ª [TEST MODE] URL 10ê°œë§Œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")

        return url_pool



    # ----------------------------------------------------------
    # STEP 5. ThreadPoolë¡œ URL ë‹¤ìš´ë¡œë“œ
    # - shuffleëœ URLì„ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ
    # - 0.3~1.2ì´ˆ sleepìœ¼ë¡œ polite crawling
    # ----------------------------------------------------------
    def download_task(self, item: dict) -> dict | None:
        publisher, url, url_hash = item["publisher"], item["url"], item["url_hash"]
        try:
            time.sleep(random.uniform(0.3, 1.2))
            html = self.html_downloader.download(url)
            filepath = self.html_downloader.save(publisher, url_hash, html)
            return {
                "timestamp": self.timestamp,
                "publisher": publisher,
                "url": url,
                "filepath": str(filepath),
                "url_hash": url_hash
            }
        except Exception as e:
            print(f"  âœ— ì‹¤íŒ¨: {url[:80]} - {e}")
            return None


    # ----------------------------------------------------------
    # STEP 6. Redis burst ì—…ë°ì´íŠ¸ (500ê°œ ë‹¨ìœ„)
    # ----------------------------------------------------------
    def run(self) -> None:
        print(f"ğŸš€ ë‹¤ìš´ë¡œë” ì‹œì‘: {self.timestamp}")
        print(f"ğŸ”§ í™˜ê²½: {'Docker' if self.config.is_docker else 'ë¡œì»¬'}")

        start_time = time.perf_counter()
        url_pool = self.collect_urls()

        pending_tasks = []
        downloaded = 0

        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            futures = [executor.submit(self.download_task, item) for item in url_pool]
            for i, future in enumerate(as_completed(futures), 1):
                result = future.result()
                if result:
                    pending_tasks.append(result)
                    downloaded += 1
                    if len(pending_tasks) >= 500:
                        self.redis_manager.flush_burst(pending_tasks)
                        pending_tasks.clear()
                    if i % 20 == 0:
                        print(f"  ì§„í–‰ë¥ : {i}/{len(url_pool)} ({downloaded} ì„±ê³µ)")

        # ë‚¨ì€ ë°ì´í„° ìµœì¢… flush
        self.redis_manager.flush_burst(pending_tasks)
        elapsed = time.perf_counter() - start_time

        if downloaded > 0:
            self.redis_manager.send_batch_complete_signal(self.timestamp, downloaded)
            print(f"\nâœ… ì™„ë£Œ ì‹ í˜¸ ì „ì†¡: {self.timestamp}")

        print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½")
        print(f"  ë‹¤ìš´ë¡œë“œ: {downloaded}ê°œ")
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        print(f"âš¡ í‰ê·  ì²˜ë¦¬ ì†ë„: {downloaded / elapsed:.2f}ê±´/ì´ˆ")


# ============================================================
# ENTRY POINT
# ============================================================

def main():
    start_http_server(8001)  # Prometheusì—ì„œ ìˆ˜ì§‘í•  /metrics ì—”ë“œí¬ì¸íŠ¸ ì˜¤í”ˆ
    timestamp = datetime.now().strftime("%Y%m%d%H")
    pipeline = NewsDownloadPipeline(timestamp)
    pipeline.run()


if __name__ == "__main__":
    main()
