# parser/main.py
import os
import json
import time
import subprocess
import shutil
from pathlib import Path
from typing import Dict, Any
from redis import Redis
from datetime import datetime, timedelta
from multiprocessing import Process
from parser_factory import ParserFactory

from prometheus_client import Counter, Summary, start_http_server

parser_tasks_processed_total = Counter("parser_tasks_processed_total", "Total processed tasks")
parser_task_failures_total = Counter("parser_task_failures_total", "Total failed tasks")
parser_s3_upload_success_total = Counter("parser_s3_upload_success_total", "S3 upload success count")
parser_s3_upload_failure_total = Counter("parser_s3_upload_failure_total", "S3 upload failure count")
parser_task_duration_seconds = Summary("parser_task_duration_seconds", "Task processing duration in seconds")
parser_idle_seconds_total = Counter("parser_idle_seconds_total", "Total idle seconds")


class ParserConfig:
    """íŒŒì„œ ì„¤ì •"""
    def __init__(self):
        self.redis_url = os.getenv('REDIS_URL', 'redis://localhost:6379')
        self.s3_bucket = os.getenv('S3_BUCKET')
        self.aws_region = os.getenv('AWS_REGION', 'ap-northeast-2')
        self.num_workers = int(os.getenv('NUM_WORKERS', '2'))
        self.is_docker = os.getenv('IS_DOCKER', '0') == '1'
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬
        if self.is_docker:
            self.temp_base = Path('/app/temp')
        else:
            self.temp_base = Path(__file__).parent.parent / 'temp'


class NewsParserService:
    """ë‰´ìŠ¤ íŒŒì„œ ì„œë¹„ìŠ¤"""
    
    def __init__(self, worker_id: int = 0):
        self.worker_id = worker_id
        self.config = ParserConfig()
        self.redis = Redis.from_url(self.config.redis_url, decode_responses=False)
        self.parser_factory = ParserFactory()
        
        self.current_batch = ""
        self.processed_count = 0
        
        print(f"[Worker {worker_id}] ğŸš€ ì‹œì‘")
    
    def process_task(self, task: dict) -> None:
        """ë‹¨ì¼ íŒŒì‹± ì‘ì—… ì²˜ë¦¬ (ë¡œì»¬ ì €ì¥ë§Œ)"""
        start_total = time.time()
        
        # ì‘ì—… ì •ë³´ ì¶”ì¶œ
        timestamp = task.get('timestamp', 'unknown')
        publisher = task.get('publisher', 'unknown')
        url = task.get('url', 'unknown')
        filepath = task.get('filepath', 'unknown')
        url_hash = task.get('url_hash', 'unknown')
        
        try:
            # ë°°ì¹˜ ì „í™˜ ì²´í¬
            if self.current_batch != timestamp:
                if self.current_batch:
                    print(f"[Worker {self.worker_id}] âœ… ë°°ì¹˜ ì „í™˜: {self.current_batch} â†’ {timestamp}")
                self.current_batch = timestamp
                self.processed_count = 0
            
            try:
                with open(filepath, "r", encoding="utf-8") as f:
                    html = f.read()
            except UnicodeDecodeError:
                if publisher == "ì‹œì‚¬ì €ë„":
                    print(f"[Worker {self.worker_id}] EUC-KR fallback ({publisher})")
                    with open(filepath, "r", encoding="euc-kr", errors="ignore") as f:
                        html = f.read()
                else:
                    raise
            
            # 2. íŒŒì‹±
            parsed = self.parser_factory.parse(html, publisher)
            
            # 3. JSON ë¡œì»¬ ì €ì¥ (S3 ì—…ë¡œë“œ ì•ˆ í•¨!)
            result = {
                'url': url,
                'publisher': publisher,
                'timestamp': timestamp,
                'parsed_at': datetime.now().isoformat(),
                **parsed
            }
            
            # parsed ë””ë ‰í† ë¦¬ì— ì €ì¥
            parsed_dir = self.config.temp_base / 'parsed' / timestamp / publisher
            parsed_dir.mkdir(parents=True, exist_ok=True)
            
            json_path = parsed_dir / f"{url_hash}.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
        
            self.processed_count += 1
            
            # ê°„ë‹¨í•œ ë¡œê·¸
            total = time.time() - start_total
            print(f"[Worker {self.worker_id}] âœ“ {publisher} | {total:.2f}ì´ˆ")
            
        except Exception as e:
            print(f"[Worker {self.worker_id}] âœ— ì‹¤íŒ¨:")
            print(f"  URL: {url[:80]}")
            print(f"  Publisher: {publisher}")
            print(f"  Error: {e}")
            import traceback
            print(f"  Traceback:\n{traceback.format_exc()}")
    
    def upload_batch_to_s3(self, timestamp: str) -> None:
        """ë°°ì¹˜ ì™„ë£Œ í›„ S3ì— ì¼ê´„ ì—…ë¡œë“œ (Worker 0ë§Œ)"""
        if self.worker_id != 0:
            return
        
        print(f"\n[Worker {self.worker_id}] ğŸ“¤ S3 ë°°ì¹˜ ì—…ë¡œë“œ ì‹œì‘: {timestamp}")
        
        try:
            # HTML ì—…ë¡œë“œ
            html_local = str(self.config.temp_base / 'html' / timestamp)
            html_s3 = f"s3://{self.config.s3_bucket}/html/{timestamp}/"
            
            if Path(html_local).exists():
                result = subprocess.run([
                    'aws', 's3', 'sync',
                    html_local,
                    html_s3,
                    '--region', self.config.aws_region,
                    '--only-show-errors'
                ], capture_output=True, text=True, timeout=300)
                
                if result.returncode == 0:
                    print(f"[Worker {self.worker_id}]   âœ“ HTML ì—…ë¡œë“œ ì™„ë£Œ")
                    # ì—…ë¡œë“œ ì„±ê³µí•˜ë©´ ë¡œì»¬ ì‚­ì œ
                    shutil.rmtree(html_local, ignore_errors=True)
                else:
                    print(f"[Worker {self.worker_id}]   âœ— HTML ì—…ë¡œë“œ ì‹¤íŒ¨: {result.stderr}")
            
            # JSON ì—…ë¡œë“œ
            parsed_local = str(self.config.temp_base / 'parsed' / timestamp)
            parsed_s3 = f"s3://{self.config.s3_bucket}/parsed/{timestamp}/"
            
            if Path(parsed_local).exists():
                result = subprocess.run([
                    'aws', 's3', 'sync',
                    parsed_local,
                    parsed_s3,
                    '--region', self.config.aws_region,
                    '--only-show-errors'
                ], capture_output=True, text=True, timeout=300)
                
                if result.returncode == 0:
                    print(f"[Worker {self.worker_id}]   âœ“ JSON ì—…ë¡œë“œ ì™„ë£Œ")
                    parser_s3_upload_success_total.inc()
                    # ì—…ë¡œë“œ ì„±ê³µí•˜ë©´ ë¡œì»¬ ì‚­ì œ
                    shutil.rmtree(parsed_local, ignore_errors=True)
                else:
                    print(f"[Worker {self.worker_id}]   âœ— JSON ì—…ë¡œë“œ ì‹¤íŒ¨: {result.stderr}")
                    parser_s3_upload_failure_total.inc()
            
            print(f"[Worker {self.worker_id}]   âœ“ ë°°ì¹˜ ì—…ë¡œë“œ ë° ì •ë¦¬ ì™„ë£Œ")
            
        except subprocess.TimeoutExpired:
            print(f"[Worker {self.worker_id}]   âœ— S3 ì—…ë¡œë“œ íƒ€ì„ì•„ì›ƒ (5ë¶„ ì´ˆê³¼)")
        except Exception as e:
            print(f"[Worker {self.worker_id}]   âœ— S3 ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
    
    def check_batch_complete(self) -> None:
        """ë°°ì¹˜ ì™„ë£Œ ì‹ í˜¸ í™•ì¸ ë° S3 ì—…ë¡œë“œ (Worker 0ë§Œ)"""
        if self.worker_id != 0 or not self.current_batch:
            return
        
        signal_key = f"batch_complete:{self.current_batch}"
        signal = self.redis.get(signal_key)
        
        if signal:
            signal_data = json.loads(signal.decode('utf-8'))
            print(f"\n[Worker {self.worker_id}] ğŸ‰ ë°°ì¹˜ ì™„ë£Œ: {self.current_batch}")
            print(f"   ë‹¤ìš´ë¡œë“œ: {signal_data['total_tasks']}ê°œ")
            print(f"   íŒŒì‹±: {self.processed_count}ê°œ")
            
            # S3 ë°°ì¹˜ ì—…ë¡œë“œ
            self.upload_batch_to_s3(self.current_batch)
            
            # ì‹ í˜¸ ì‚­ì œ
            self.redis.delete(signal_key)
            self.current_batch = ""
            self.processed_count = 0
    
    def run(self) -> None:
        """ì„œë¹„ìŠ¤ ì‹¤í–‰ - Redis í êµ¬ë…"""
        print(f"[Worker {self.worker_id}] â³ Redis í ëŒ€ê¸° ì¤‘...\n")
        
        consecutive_empty = 0
        
        while True:
            try:
                # Pipelineìœ¼ë¡œ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°
                pipe = self.redis.pipeline()
                batch_size = 20
                
                for _ in range(batch_size):
                    pipe.rpop('parse_queue')
                
                results = pipe.execute()
                tasks = [json.loads(r.decode('utf-8')) for r in results if r]
                
                if tasks:
                    consecutive_empty = 0
                    print(f"[Worker {self.worker_id}] ğŸ“¦ ë°°ì¹˜: {len(tasks)}ê°œ")
                    
                    for task in tasks:
                        self.process_task(task)
                else:
                    consecutive_empty += 1
                    parser_idle_seconds_total.inc()

                    if consecutive_empty == 1:
                        self.check_batch_complete()  # S3 ì—…ë¡œë“œ ì—¬ê¸°ì„œ!
                    
                    if consecutive_empty % 10 == 0:
                        print(f"[Worker {self.worker_id}] ğŸ’¤ ëŒ€ê¸° ì¤‘... ({consecutive_empty}ì´ˆ)")
                    
                    time.sleep(1)
                    
                    if consecutive_empty % 600 == 0 and self.worker_id == 0:
                        cleanup_old_dirs(self.config.temp_base / 'html', hours=24)
                        cleanup_old_dirs(self.config.temp_base / 'parsed', hours=24)

            except KeyboardInterrupt:
                print(f"\n[Worker {self.worker_id}] ğŸ›‘ ì¢…ë£Œ")
                break
            except Exception as e:
                print(f"[Worker {self.worker_id}] âŒ ì˜¤ë¥˜: {e}")
                time.sleep(1)


def worker_process(worker_id: int):
    """ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ í•¨ìˆ˜"""
    service = NewsParserService(worker_id)
    service.run()

def cleanup_old_dirs(base_path: Path, hours: int = 24):
    """ì§€ì • ì‹œê°„(hours) ì´ìƒ ì§€ë‚œ ë°°ì¹˜ ë””ë ‰í† ë¦¬ ìë™ ì •ë¦¬"""
    cutoff = datetime.now() - timedelta(hours=hours)
    for subdir in base_path.iterdir():
        if not subdir.is_dir():
            continue
        try:
            ts = datetime.strptime(subdir.name, "%Y%m%d_%H")
            if ts < cutoff:
                shutil.rmtree(subdir, ignore_errors=True)
                print(f"ğŸ§¹ ì˜¤ë˜ëœ ë””ë ‰í† ë¦¬ ì‚­ì œ: {subdir}")
        except ValueError:
            continue

def main():
    config = ParserConfig()
    num_workers = config.num_workers
    start_http_server(8002)  # Prometheusì—ì„œ ìˆ˜ì§‘í•  /metrics ì—”ë“œí¬ì¸íŠ¸ ì˜¤í”ˆ
    
    print(f"ğŸš€ Parser ì„œë¹„ìŠ¤ ì‹œì‘ ({num_workers} workers)")
    print(f"ğŸ”§ í™˜ê²½: {'Docker' if config.is_docker else 'ë¡œì»¬'}")
    print(f"ğŸ“¦ S3 ë²„í‚·: {config.s3_bucket}")
    print(f"ğŸ“‚ ì„ì‹œ ë””ë ‰í† ë¦¬: {config.temp_base}\n")
    
    # ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì‹œì‘
    processes = []
    for i in range(num_workers):
        p = Process(target=worker_process, args=(i,))
        p.start()
        processes.append(p)
    
    # ëª¨ë“  ì›Œì»¤ ëŒ€ê¸°
    try:
        for p in processes:
            p.join()
    except KeyboardInterrupt:
        print("\nğŸ›‘ ëª¨ë“  ì›Œì»¤ ì¢…ë£Œ ì¤‘...")
        for p in processes:
            p.terminate()
            p.join()


if __name__ == '__main__':
    main()